{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CSF1-targeted chimeric antigen receptor activates CSF1R signalling and effects growth in THP-1 monocytes\n",
    "\n",
    "Aurora Callahan<sup>1</sup>, Amber Wang<sup>1</sup>, Aisharja Mojumdar<sup>1</sup>, Xinyan Zhang<sup>2</sup>, Longhui Zeng<sup>2</sup>, Xiaolei Su<sup>2,*</sup>, Arthur R. Salomon<sup>1,*</sup>\n",
    "\n",
    "<sup>1</sup> Department of Molecular Biology, Cell Biology, and Biochemistry, Brown University, Providence, RI, 02903, USA\n",
    "\n",
    "<sup>2</sup> Department of Cell Biology, Yale School of Medicine, Yale University, New Haven, CT, 06520, USA\n",
    "\n",
    "<sup>*</sup> Corresponding Authors: xiaolei.su@yale.edu, art@drsalomon.com\n",
    "\n",
    "<strong>Abstract </strong>\n",
    "\n",
    "A wave of reports evaluating the use of chimeric antigen receptor (CAR) T cells for clearance of tumour infiltrating macrophages (TAMs), which assist growing tumours in immune evasions and immune suppression, show that CAR-mediated TAM clearance improves immune responses towards tumours. Colony stimulating factor 1 receptor (CSF1R) is an attractive target for TAM clearance due the necessity of CSF1R for macrophage differentiation and survival. Previous reports use CSF1, the native ligand of CSF1R, as the targeting mechanism for CAR T cells and found that CARs with this arrangement can lyse CSF1R overexpressing lymphomas, however there is currently no mechanistic evaluation of ligand-targeted CARs. Using phosphotyrosine (pY) enrichment coupled with LC-MS/MS, we evaluate the signalling capacity of a second generation CSF1-targeted CSF1R-CAR compared with a scFv-targeted CD19-CAR. We find that CSF1R-CAR T cells do not engage TCR signalling with the same strength as CD19-CAR T cells, however T cell receptor signalling pY sites are significantly enriched after CAR-target ligation in both CSF1R- and CD19-CAR T cells. We show that ligation of CSF1R-CAR T cells with CSF1R-expressing THP-1 monocytes induces CSF1R-like signalling, whereas no signalling response is observed after CD19-CAR/Raji B cell ligation. Using small molecule inhibitors of Lck, actin polymerisation, and CSF1R, we find that CAR-induced CSF1R signalling depends exclusively on CSF1R kinase activity with no participation from T cell activation. Our data provide evidence for an unintended consequence of ligand-targeted CARs, which may have implications for the clinical efficacy of ligand-targeted CARs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Notebook Synopsis</strong> \n",
    "\n",
    "This notebook takes the Excel output from the high throughput autonomous proteomics pipeline (HTAPP)<a href=\"https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/full/10.1002/pmic.200900159?casa_token=ZI5Ml2QqE7sAAAAA%3AxXrc3KZbzG1iyRIi3afNoHXGJA6cc0Ot9fYuiYPUOJP88zd2XJPYBsjiL7b8Oezh1GJUI4dKZpYvgA\">[1]</a> and PeptideDepot<a href=\"https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/full/10.1002/pmic.200900119?casa_token=BzpTzA0oeHYAAAAA%3AcHtUnF72XzUz4DslIx6Eq0BpMuDxryHPR43APtNnittn1Ry-rD5UqsxAWo1Bg_PnBX4KB4WHtRODiA\">[2]</a> developed by the Salomon Laboratory and produces the graphs used in the above manuscript. Notes regarding the steps will be shown above each code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necesary dependencies, including Aurora's 'helpers' package that includes basic data processing functions, statistical functions, plotting functions, and proteomics plotting functions. The helpers functions print the versions of dependent packages when they run. The files py_scripts/generate_ssgsea_files and py_scripts/managing_ssgsea_outputs are from <a href=\"https://pubs.acs.org/doi/full/10.1021/acs.jproteome.1c00735?casa_token=cCHo8ZEvMPgAAAAA%3APgG2SoOHCcf9tFLOdsprVzLcesjK-jh2Cu5KAifl_ogmdcTp0LVBj7TYvQ_nMW8Aq_y2JOpqp4zFAQ\">[3]</a> and are used to take the files from ssGSEA2.0.R <a href=\"https://www.mcponline.org/article/S1535-9476(20)31860-0/fulltext\">[4]</a> and put them into a workable format for plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importables\n",
    "from platform import python_version\n",
    "print(f\"Python version {python_version()}\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#sys.path.append(\"/windir/c/Users/redas/Desktop/jupyter_directory/helpers/src/helpers/\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from math import log10, log2, ceil, floor, sqrt\n",
    "\n",
    "# And grab the helpers\n",
    "import sys\n",
    "sys.path.append(\"/windir/c/Users/redas/Desktop/jupyter_directory/helpers/src/helpers/\")\n",
    "from helpers import general_helpers as gh\n",
    "from helpers import stats_helpers as sh\n",
    "from helpers import mpl_plotting_helpers as mph\n",
    "from helpers.proteomics_helpers import Peptide\n",
    "from helpers import argcheck_helpers as ah\n",
    "\n",
    "# Import scripts for processing PTM-SEA output\n",
    "from py_scripts import generate_ssgsea_files as gsf\n",
    "from py_scripts import managing_ssgsea_outputs as mso\n",
    "\n",
    "# for the enrichment dotplots, will be moved eventually\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.patches import Circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining global variables that will be used throughout the script, including file names and relative paths, experiment names for plotting, the 'real' column headers, and plotting colours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting this settled\n",
    "\n",
    "files = [\"raji_28bbz/pY_data/28BBZ_CD19CarT_Raji_heavy.xls\",          #0\n",
    "         \"raji_28z/pY_data/28Z_CD19CarT_Raji_light.xls\",            #2\n",
    "         \"thp1_28z/pY_data/28Z_CSF1car_THP1_light.xls\",             #4\n",
    "         \"28bbz_raji/pY_data/28BBZ_Raji_CD19CarT_light.xls\",          #1\n",
    "         \"28z_raji/pY_data/28Z_Raji_CD19CarT_heavy.xls\",            #3\n",
    "         \"28z_thp1/pY_data/28Z_THP1_CSF1RCar_heavy.xls\"]            #5\n",
    "\n",
    "ptm_files = [\"./raji_28bbz/pY_data/output_combined_heatmap.txt\",\n",
    "             \"./raji_28z/pY_data/output_combined_heatmap.txt\",\n",
    "             \"./thp1_28z/pY_data/output_combined_heatmap.txt\",\n",
    "             \"./28bbz_raji/pY_data/output_combined_heatmap.txt\",\n",
    "             \"./28z_raji/pY_data/output_combined_heatmap.txt\",\n",
    "             \"./28z_thp1/pY_data/output_combined_heatmap.txt\",\n",
    "            ]\n",
    "\n",
    "\n",
    "target_inds = [1,2]\n",
    "car_inds = [4,5]\n",
    "\n",
    "thirdgen_repeat_inds = [[0],\n",
    "                        [3]]\n",
    "\n",
    "outfiles = [[\"28BBZ_CD19CarT_Raji_heavy_2m.pdf\", \"28BBZ_CD19CarT_Raji_heavy_5m.pdf\"],\n",
    "            [\"28Z_CD19CarT_Raji_light_2m.pdf\", \"28Z_CD19CarT_Raji_light_5m.pdf\"],\n",
    "            [\"28Z_CSF1car_THP1_light_2m.pdf\", \"28Z_CSF1car_THP1_light_5m.pdf\"],\n",
    "            [\"28BBZ_Raji_CD19CarT_light_2m.pdf\", \"28BBZ_Raji_CD19CarT_light_5m.pdf\"],\n",
    "            [\"28Z_Raji_CD19CarT_heavy_2m.pdf\", \"28Z_Raji_CD19CarT_heavy_5m.pdf\"],\n",
    "            [\"28Z_THP1_CSF1RCar_heavy_2m.pdf\", \"28Z_THP1_CSF1RCar_heavy_5m.pdf\"]]\n",
    "\n",
    "exp_names = [r\"28BB$\\zeta$-CAR (CD19) + Raji (Raji pY Sites)\",\n",
    "            r\"28$\\zeta$-CAR (CD19) + Raji (Raji pY Sites)\",\n",
    "            r\"28$\\zeta$-CAR (CSF1R) + THP-1 (THP-1 pY Sites)\",\n",
    "             r\"28BB$\\zeta$-CAR (CD19) + Raji (CAR pY Sites)\",\n",
    "            r\"28$\\zeta$-CAR (CD19) + Raji (CAR pY Sites)\",\n",
    "            r\"28$\\zeta$-CAR (CSF1R) + THP-1 (CAR pY Sites)\"]\n",
    "\n",
    "xaxis_strs = [\"2 min vs 0 min\",\n",
    "              \"5 min vs 0 min\"]\n",
    "\n",
    "yaxis_strs = [r\"$-\\log_{10}(q)$\",\n",
    "              r\"$-\\log_{10}(q)$\"]\n",
    "\n",
    "columns = [\"UNIPROT Gene Name\",                                #0\n",
    "           \"phosphosite annotated\",                            #1\n",
    "            \"peakarea manual 1 rep1 thresholded timepoint1\",   #2\n",
    "            \"peakarea manual 1 rep2 thresholded timepoint1\",   #3\n",
    "            \"peakarea manual 1 rep3 thresholded timepoint1\",   #4\n",
    "            \"peakarea manual 1 rep4 thresholded timepoint1\",   #5\n",
    "            \"peakarea manual 1 rep1 thresholded timepoint2\",   #6\n",
    "            \"peakarea manual 1 rep2 thresholded timepoint2\",   #7\n",
    "            \"peakarea manual 1 rep3 thresholded timepoint2\",   #8\n",
    "            \"peakarea manual 1 rep4 thresholded timepoint2\",   #9\n",
    "            \"peakarea manual 1 rep1 thresholded timepoint3\",   #10\n",
    "            \"peakarea manual 1 rep2 thresholded timepoint3\",   #11\n",
    "            \"peakarea manual 1 rep3 thresholded timepoint3\",   #12\n",
    "            \"peakarea manual 1 rep4 thresholded timepoint3\",   #13\n",
    "            \"SILAC ratio 23 for user selected SILAC timepoint1\",#14\n",
    "            \"SILAC ratio 23 for user selected SILAC timepoint2\",#15\n",
    "            \"qvalues for SILAC timepoint1\",                    #16\n",
    "            \"qvalues for SILAC timepoint2\",                    #17\n",
    "            \"assigned sequence\",                               #18\n",
    "            \"Kegg unique index\",                               #19\n",
    "            \"peptide sequence GCT format centered on 1st site\",#20\n",
    "            \"peptide sequence GCT format centered on 2nd site\",#21\n",
    "            \"peptide sequence GCT format centered on 3rd site\" #22\n",
    "           ]\n",
    "\n",
    "fcond_cols = [\"UNIPROT Gene Name\",                                #0\n",
    "           \"phosphosite annotated\",                            #1\n",
    "            \"peakarea manual 1 rep1 thresholded timepoint1\",   #2\n",
    "            \"peakarea manual 1 rep2 thresholded timepoint1\",   #3\n",
    "            \"peakarea manual 1 rep3 thresholded timepoint1\",   #4\n",
    "            \"peakarea manual 1 rep4 thresholded timepoint1\",   #5\n",
    "            \"peakarea manual 1 rep1 thresholded timepoint2\",   #6\n",
    "            \"peakarea manual 1 rep2 thresholded timepoint2\",   #7\n",
    "            \"peakarea manual 1 rep3 thresholded timepoint2\",   #8\n",
    "            \"peakarea manual 1 rep4 thresholded timepoint2\",   #9\n",
    "            \"peakarea manual 1 rep1 thresholded timepoint3\",   #10\n",
    "            \"peakarea manual 1 rep2 thresholded timepoint3\",   #11\n",
    "            \"peakarea manual 1 rep3 thresholded timepoint3\",   #12\n",
    "            \"peakarea manual 1 rep4 thresholded timepoint3\",   #13\n",
    "            \"SILAC ratio 23 for user selected SILAC timepoint1\",#14\n",
    "            \"SILAC ratio 23 for user selected SILAC timepoint2\",#15\n",
    "            \"qvalues for SILAC timepoint1\",                    #16\n",
    "            \"qvalues for SILAC timepoint2\",                    #17\n",
    "            \"assigned sequence\",                               #18\n",
    "            \"Kegg unique index\",                               #19\n",
    "            \"flank1\"\n",
    "           ]\n",
    "\n",
    "newcol = [\"Gene\",\n",
    "          \"Phosphorylation Site\", \n",
    "          \"0m R1\", \"0m R2\", \"0m R3\", \"0m R4\",\n",
    "          \"2m R1\", \"2m R2\", \"2m R3\", \"2m R4\",\n",
    "          \"5m R1\", \"5m R2\", \"5m R3\", \"5m R4\",\n",
    "          \"2m vs 0m Foldchange\", \"5m vs 0m Foldchange\",\n",
    "          \"2m vs 0m qvalue\", \"5m vs 0m qvalue\", \"Sequence\",\n",
    "          \"KEGG\", \"Flank 1\", \"Flank 2\", \"Flank 3\"]\n",
    "rename = {columns[i]:newcol[i] for i in range(len(columns))}\n",
    "\n",
    "\n",
    "\n",
    "colours = [mph.handle_colours(\"purples\", 3),      #28bbz + Raji 5m rest, Raji\n",
    "           mph.handle_colours(\"blues\", 3),      #28z + Raji 30m rest, Raji\n",
    "           mph.handle_colours(\"pinks\", 3),    #28z + THP1 30m rest, THP1\n",
    "           mph.handle_colours(\"purples\", 3),      #28bbz + Raji 5m rest, CAR\n",
    "           mph.handle_colours(\"blues\", 3),      #28z + Raji 30m rest, CAR\n",
    "           mph.handle_colours(\"pinks\", 3)]    #28z + THP1 30m rest, CAR\n",
    "\n",
    "# Variables needed for volcano plots\n",
    "sigs = [0.005,0.01,0.05] \n",
    "rightlabs = [\"2 min vs 0 min\", \"5 min vs 0m\"]\n",
    "leftlabs = [fr\"$-\\log_{{10}}(q)$\" for i in range(2)]\n",
    "bottomlabs = [fr\"$\\log_{{2}}(t_{{i}})- \\log_{{2}}({{t_{{0}}}})$\" for i in range(2)]\n",
    "toplabs = [[ fr\"Raji (CD19-28BB$\\zeta$)\"],\n",
    "           [  fr\"Raji (CD19-28$\\zeta$)\"],\n",
    "           [  fr\"THP-1 (CSF1R-28$\\zeta$)\"],\n",
    "           [  fr\"CD19-28BB$\\zeta$ (Raji)\"],\n",
    "           [  fr\"CD19-28$\\zeta$ (Raji)\"],\n",
    "           [  fr\"CSF1R-28$\\zeta$ (THP-1)\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File paths for outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gct_outnames =  [\"raji_28bbz/pY_data/qvalue.gct\",          #0\n",
    "         \"raji_28z/pY_data/qvalue.gct\",            #2\n",
    "         \"thp1_28z/pY_data/qvalue.gct\",             #4\n",
    "         \"28bbz_raji/pY_data/qvalue.gct\",          #1\n",
    "         \"28z_raji/pY_data/28Z_qvalue.gct\",            #3\n",
    "         \"28z_thp1/pY_data/qvalue.gct\"]            #5\n",
    "\n",
    "reg_outnames = [\"raji_28bbz/28BBZ_CD19CarT_Raji_h_reg.pdf\",\n",
    "                \"raji_28z/28Z_CD19CarT_Raji_l_reg.pdf\",\n",
    "                \"thp1_28z/28Z_CSF1RCarT_THP1_l_reg.pdf\",\n",
    "                \"28bbz_raji/28BBZ_CD19CarT_Raji_l_reg.pdf\",\n",
    "                \"28z_raji/28Z_CD19CarT_Raji_h_reg.pdf\",\n",
    "                \"28z_thp1/28Z_CSF1RCarT_THP1_h_reg.pdf\",]\n",
    "\n",
    "pca_outnames = [\"raji_28bbz/28BBZ_CD19CarT_Raji_h_pca.pdf\",\n",
    "                \"raji_28z/28Z_CD19CarT_Raji_l_pca.pdf\",\n",
    "                \"thp1_28z/28Z_CSF1RCarT_THP1_l_pca.pdf\",\n",
    "                \"28bbz_raji/28BBZ_CD19CarT_Raji_l_pca.pdf\",\n",
    "                \"28z_raji/28Z_CD19CarT_Raji_h_pca.pdf\",\n",
    "                \"28z_thp1/28Z_CSF1RCarT_THP1_h_pca.pdf\",]\n",
    "\n",
    "nmf_outnames = [\"raji_28bbz/28BBZ_CD19CarT_Raji_h_nmf.pdf\",\n",
    "                \"raji_28z/28Z_CD19CarT_Raji_l_nmf.pdf\",\n",
    "                \"thp1_28z/28Z_CSF1RCarT_THP1_l_nmf.pdf\",\n",
    "                \"28bbz_raji/28BBZ_CD19CarT_Raji_l_nmf.pdf\",\n",
    "                \"28z_raji/28Z_CD19CarT_Raji_h_nmf.pdf\",\n",
    "                \"28z_thp1/28Z_CSF1RCarT_THP1_h_nmf.pdf\",]\n",
    "\n",
    "volc_outnames = [\"raji_28bbz/volcanoes.pdf\",\n",
    "                \"raji_28z/volcanoes.pdf\",\n",
    "                \"thp1_28z/volcanoes.pdf\",\n",
    "                \"28bbz_raji/volcanoes.pdf\",\n",
    "                \"28z_raji/volcanoes.pdf\",\n",
    "                \"28z_thp1/volcanoes.pdf\",]\n",
    "\n",
    "site_outnames = [\"raji_28bbz/sites\",\n",
    "                \"raji_28z/sites\",\n",
    "                \"thp1_28z/sites\",\n",
    "                \"28bbz_raji/sites\",\n",
    "                \"28z_raji/sites\",\n",
    "                \"28z_thp1/sites\",]\n",
    "\n",
    "outdirs = [\"raji_28bbz/pY_data\",\n",
    "                \"raji_28z/pY_data\",\n",
    "                \"thp1_28z/pY_data\",\n",
    "                \"28bbz_raji/pY_data\",\n",
    "                \"28z_raji/pY_data\",\n",
    "                \"28z_thp1/pY_data\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for transforming values and writing GCT files used in PTM-SEA. Specifics of the functions are listed in the documentation strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GCT file writing\n",
    "## Need to keep only the input values and flanks\n",
    "\n",
    "def ptmsea_transform(a_qvalue, a_foldchange, head = \"No\"):\n",
    "    \"\"\"\n",
    "    This function takes a single q-value (or p-value) and the foldchange, and performs the following transformation:\n",
    "    -log10(q)*(-1) if the foldchange is between 0 and 1\n",
    "    -log10(q) if the foldchange is greater than 1\n",
    "    Assumes the foldchanges are not log transformed\n",
    "    \"\"\"\n",
    "    if type(a_qvalue) == str or type(a_foldchange) == str:\n",
    "        return head\n",
    "    if a_qvalue != a_qvalue or a_foldchange != a_foldchange:\n",
    "        return float(\"nan\")\n",
    "    if 0 < a_foldchange < 1:\n",
    "        return -10 * log10(a_qvalue) *-1\n",
    "    else:\n",
    "        return -10 * log10(a_qvalue) * 1\n",
    "\n",
    "def duplicate_flanks(a_row,\n",
    "                     flank_locs,   # list of indices for the places with flanking sequences\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    This function takes the n-columns defining flanking sequences in a row and returns new rows where\n",
    "    all the values are duplicated but contain a single flanking sequence. Builds new lists iteratively.\n",
    "\n",
    "    Ex input: [Zap70, S491Y492Y493, KALGADDSYYTARSA , ALGADDSYYTARSAG, LGADDSYYTARSAGK] \n",
    "    Ex output: [[Zap70, S491Y492Y493, KALGADDSYYTARSA] ,\n",
    "                [Zap70, S491Y492Y493, ALGADDSYYTARSAG] ,\n",
    "                [Zap70, S491Y492Y493, LGADDSYYTARSAGK] ]\n",
    "    \"\"\"\n",
    "    flank_num = len(flank_locs)\n",
    "    new_rows = [[] for _ in range(flank_num)] # Make the new rows\n",
    "    flanks = []\n",
    "    f_ind = -1\n",
    "    seen = False\n",
    "    for i in range(len(a_row)):               # loop over the size of the row\n",
    "        if i not in flank_locs:               # If this is not a flank\n",
    "            for j in range(flank_num):\n",
    "                new_rows[j].append(a_row[i])  # Add this info to each sublist\n",
    "        elif i in flank_locs:                 # If you hit a flank index\n",
    "            flanks.append(a_row[i])           # Add the flank to the flanks list\n",
    "            if not seen:                      # if it's new\n",
    "                for j in range(flank_num):    # Then add a placeholder \n",
    "                    new_rows[j].append(\"Flankity Flank Flank\")\n",
    "                seen = True                   # and mark as seen\n",
    "            if f_ind == -1:                   # and update the flank index\n",
    "                f_ind = i\n",
    "    for i in range(flank_num):\n",
    "        new_rows[i][f_ind] = flanks[i]        # Then add the flanks back in\n",
    "    return new_rows\n",
    "\n",
    "def dup_flanks_matrix(a_matrix, flank_locs):\n",
    "    \"\"\"\n",
    "    Wraps duplicate_flanks for every row in a matrix, building a new matrix with the single flank columns\n",
    "    \"\"\"\n",
    "    new_matr = []\n",
    "    for row in a_matrix:\n",
    "        exp = duplicate_flanks(row, flank_locs)\n",
    "        new_matr += exp\n",
    "    return new_matr\n",
    "\n",
    "def write_gct_file(index,    # Flanking sequences with -p \n",
    "                    values,   # values per flanking sequence, len(values[i]) == len(headers), len(values) == len(index)\n",
    "                    headers, # Headers for the columns\n",
    "                    outfile = \"bullshit.gct\" # File to write, include path and .gct\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Writes a GCT file given the index of properly formatted flanking sequences (index), the values for each flanking sequence,\n",
    "    and the headers. len(header) == len(values[i])\n",
    "\n",
    "    returns None, writes a file to the given path instead. \n",
    "    \"\"\"\n",
    "    # GCT file has some info up top, we take it\n",
    "    gct = [[fr\"#1.3\"], # Tells programs its a GCT 1.3 file\n",
    "           [len(index), len(headers), 0, 0], # number of rows, number of cols, and metadata shit\n",
    "           [\"flanking_seq\"] + headers] \n",
    "    for i in range(len(index)):\n",
    "        gct.append([index[i]] + values[i])\n",
    "    gct = [gh.list_to_str(row) for row in gct]\n",
    "    gh.write_outfile(gct, outfile, writestyle = \"w\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for processing input files, including filtering the rows of a dataframe to remove redundancy, reading and fitlering the files, and finding the limits for plotting later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_rows(a_df, column = \"U_ID\"):\n",
    "    \"\"\"\n",
    "    Takes the rows of a PeptideDepot xls file (as a Pandas DataFrame) and a unique id, then filters the dataframe\n",
    "    to include only one row with that unique id. The dataframe should be pre-sorted.\n",
    "    \"\"\"\n",
    "    cols = list(a_df.columns)\n",
    "    keycol = cols.index(column)\n",
    "    a_list = list(a_df.to_numpy())\n",
    "    first_occ = []\n",
    "    first_key = []\n",
    "    for row in a_list:\n",
    "        if row[keycol] not in first_key:\n",
    "            first_occ.append(row)\n",
    "            first_key.append(row[keycol])\n",
    "        else:\n",
    "            pass\n",
    "    return pd.DataFrame(first_occ, columns = cols)\n",
    "\n",
    "def read_and_filter(a_file, columns):\n",
    "    \"\"\"\n",
    "    Given an xls file (from peptide depot) and a list of columns for that file, filter the dataframe for \n",
    "    further processing. \n",
    "    \"\"\"\n",
    "    file = pd.read_excel(a_file)\n",
    "    file = file[columns]\n",
    "    file[\"U_ID\"] = file[columns[0]] + file[columns[1]]\n",
    "    fcs = list(file[columns[14:16]].astype(float).to_numpy())\n",
    "    fcs = [[abs(value) for value in row] for row in fcs]\n",
    "    qs = list(file[columns[16:18]].astype(float).to_numpy())\n",
    "    # Originally replaced NANs with 1 in qs, but that's stupid I think.\n",
    "    #qs = [replace_nans(row) for row in qs]\n",
    "    #qs = [replcae_value(row, float('nan'), )]\n",
    "    file[\"q_num\"] = [sum([1 for _ in row if _ < 0.05]) for row in qs]\n",
    "    file[\"min_q\"] = [min(row) for row in qs]\n",
    "    file[\"max_fc\"] = [max(row) for row in fcs]\n",
    "    file = file.sort_values([\"U_ID\", \"q_num\", \"min_q\", \"max_fc\"],\n",
    "                            ascending = [True, False, True, False])\n",
    "    file = filter_df_rows(file)\n",
    "    return file \n",
    "\n",
    "def find_lims(all_qs,all_fcs):\n",
    "    \"\"\"\n",
    "    Given a list of all q-values and all fold changes, return the max -log10(q) and\n",
    "    foldchange (assumes foldchanges are log transformed)\n",
    "    \"\"\"\n",
    "    qs = gh.unpack_list(all_qs)\n",
    "    fc = [abs(item) for item in gh.unpack_list(all_fcs) if item == item]\n",
    "    return round(-log10(min(qs)))+1, round(max(fc))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for performing principal component analysis (PCA) and non-negative matrix factorisation (NMF) for clustering analysis and plotting the results in a 2D graph. These functions wrap the scikitlearn PCA and NMF objects. This section will eventually be moved entirely to mpl_plotting_helpers because of how often I do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA function to make my life less miserable\n",
    "def square_bounds(mpl_axes, inplace = False):\n",
    "    \"\"\"\n",
    "    This just finds the biggest difference and creates square axes, which is just\n",
    "    nicer sometimes.\n",
    "    \"\"\"\n",
    "    ticks = list(mpl_axes.get_xticks()) + list(mpl_axes.get_yticks())\n",
    "    if inplace:\n",
    "        mpl_axes.set_xlim(min(ticks), max(ticks))\n",
    "        mpl_axes.set_ylim(min(ticks), max(ticks))\n",
    "    else:\n",
    "        return min(ticks), max(ticks)\n",
    "\n",
    "def pca_analysis(a_df, pca_kwargs = dict(n_components = 2,\n",
    "                                         whiten = False,\n",
    "                                         svd_solver = \"full\",\n",
    "                                         tol = 0)):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a dictionary with keyword arguments for scikit learns PCA,\n",
    "    perform dimenstionality reduction and reutrn the components and the PCA object.\n",
    "\n",
    "    Note that the components contain the coordinates for the dimensionally reduced\n",
    "    points.\n",
    "    \"\"\"\n",
    "    std_scalar = StandardScaler()\n",
    "    scaled = std_scalar.fit_transform(a_df.transpose())\n",
    "    pca_analysis = PCA(**pca_kwargs)\n",
    "    components = pca_analysis.fit_transform(scaled)\n",
    "    components = gh.transpose(*[list(point) for point in list(components)])\n",
    "    return components, pca_analysis\n",
    "\n",
    "def nmf_analysis(a_df, nmf_kwargs = dict(n_components = 2, \n",
    "                                     init = \"nndsvd\", # preserves sparseness\n",
    "                                     solver = \"mu\", # multiplicative update\n",
    "                                     beta_loss = \"frobenius\", # stable, but slow\n",
    "                                     alpha_W = 0,  # default\n",
    "                                     alpha_H = 0,  # default\n",
    "                                     l1_ratio = 0  # default\n",
    "                                    )):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a dictionary with keyword arguments for scikit learns NMF,\n",
    "    perform dimenstionality reduction and reutrn the components and the NMF object.\n",
    "\n",
    "    Note that the components contain the coordinates for the dimensionally reduced\n",
    "    points.\n",
    "    \"\"\"\n",
    "    #std_scalar = StandardScaler()\n",
    "    #scaled = std_scalar.fit_transform(a_df.transpose())\n",
    "    norms = normalize(a_df)\n",
    "    nmf_analysis = NMF(**nmf_kwargs)\n",
    "    W = nmf_analysis.fit_transform(norms)\n",
    "    H = nmf_analysis.components_\n",
    "    return H, W\n",
    "    \n",
    "def cluster_plotting(dataframes, # list with minimum 1 df\n",
    "                 groups,     \n",
    "                 expnames, # should match len(dataframes)\n",
    "                 filenames,# should match len(dataframes)\n",
    "                 group_slices, #assumes reps are clustered in list\n",
    "                 labels,       # should correspons to len(group_slices)\n",
    "                 colours,      # list of lists, each sublist should correspond to len(group_slices)\n",
    "                 markers = [\"o\",\"^\", \"s\"],\n",
    "                 cluster = 'PCA', # other option is NNMF\n",
    "                 markersize=100, \n",
    "                 textdict = dict(fontfamily = \"sans-serif\",\n",
    "                 font = \"Arial\",\n",
    "                 fontweight = \"bold\",\n",
    "                 fontsize = 10),\n",
    "                 pca_kwargs = dict(n_components = 2,\n",
    "                                   whiten = False,\n",
    "                                   svd_solver = \"full\",\n",
    "                                   tol = 0),\n",
    "                 nmf_kwargs = dict(n_components = 2, \n",
    "                                   init = \"nndsvd\", # preserves sparseness\n",
    "                                   solver = \"mu\", # multiplicative update\n",
    "                                   beta_loss = \"frobenius\", # stable, but slow\n",
    "                                   alpha_W = 0,  # default\n",
    "                                   alpha_H = 0,  # default\n",
    "                                   l1_ratio = 0  # default\n",
    "                                    )):\n",
    "    \"\"\"\n",
    "    Function that wraps PCA or NMF analysis and makes a 2D plot with my specific graphing style\n",
    "    \"\"\"\n",
    "    # Get the data columns from the dataframes, remove\n",
    "    # missing values, run PCA analysis with sklearn,\n",
    "    # scatter\n",
    "    \n",
    "    # Grab the columns corresponding to the groups of data. Assumes the 'groups' strings\n",
    "    # are a substring of the column headers\n",
    "    dfs = [df[[name for name in list(df.columns) if any(s in name for s in groups)]] for df in dataframes]\n",
    "    # Remove any row with missing values, as PCA doesn't tolerate MVs\n",
    "    dfs = [df.dropna() for df in dfs]\n",
    "    axes = []\n",
    "    i = 0\n",
    "    for df in dfs:\n",
    "        if cluster == \"PCA\":\n",
    "            components,pca = pca_analysis(df, pca_kwargs = pca_kwargs)\n",
    "        else:\n",
    "            # will add nmf soon\n",
    "            components,nmf = nmf_analysis(df, nmf_kwargs = nmf_kwargs)\n",
    "        fig, ax = plt.subplots(figsize = (6,6))\n",
    "        # Next, loop over the slices and scatter\n",
    "        j = 0\n",
    "        for g in group_slices:\n",
    "            ax.scatter(components[0][g], components[1][g], \n",
    "                       color = colours[i][j],\n",
    "                       marker = markers[j], \n",
    "                       s = markersize, \n",
    "                       alpha = 0.75,          # my preference\n",
    "                       label = labels[j],\n",
    "                       edgecolor = \"black\",   # my preference\n",
    "                      )\n",
    "            j+=1\n",
    "        ax.set_title(expnames[i], **textdict)\n",
    "        if cluster == \"PCA\":\n",
    "            ax.set_xlabel(f\"PC1 ({100*pca.explained_variance_ratio_[0]:.2f}%)\",**textdict)\n",
    "            ax.set_ylabel(f\"PC2 ({100*pca.explained_variance_ratio_[1]:.2f}%)\", **textdict)\n",
    "            #square_bounds(ax, inplace = True)\n",
    "        else:\n",
    "            # will add nmf soon\n",
    "            ax.set_xlabel(f\"Component 1\", **textdict)\n",
    "            ax.set_ylabel(f\"Component 2\", **textdict)\n",
    "        square_bounds(ax, inplace = True)\n",
    "        mph.update_ticks(ax, which = \"x\")\n",
    "        mph.update_ticks(ax, which =\"y\")\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filenames[i])\n",
    "        axes.append(ax)\n",
    "        plt.close()\n",
    "        i+=1\n",
    "    return axes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for doing regression and plotting the resulting strings. Rather than doing pairwise comparisons, we just do multiple linear regression because that's a bit more efficient. Wraps scikitlearn's LinearRegression() object with matplotlib text plotting and LaTeX formatting. This entire section will eventually be migrated to mpl_plotting_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple Regression, because fuck doing the pairwise bullshit\n",
    "reg_gs = [\"thresholded timepoint1\", \"thresholded timepoint2\", \"thresholded timepoint3\"]\n",
    "\n",
    "def format_linreg_strs(coeffs, r2, intercept, label = \"bd\"):\n",
    "    \"\"\"\n",
    "    Function that creates the equation of a line given n coefficients and an intercept.\n",
    "    \"\"\"\n",
    "    outstr = f\"{label}\\n$y={intercept:.3f}\"\n",
    "    for i in range(len(coeffs)):\n",
    "        outstr += fr\"+{coeffs[i]:.3f}x_{{{i+1}}}\"\n",
    "    outstr += f\"$\\n$R={r2:.3f}$\"\n",
    "    return outstr\n",
    "\n",
    "def plot_linreg_strs(strs, save = \"test.pdf\"):\n",
    "    \"\"\"\n",
    "    Takes the output of format_linreg_strs and plots using matplotlib and LaTeX for formatting\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    # Check how many strings there are, and adjust the axes accordingly\n",
    "    num = len(strs)\n",
    "    ax.set_xlim(-1,1)\n",
    "    ax.set_ylim(-1,num)\n",
    "    ax.set_yticks(list(range(num)))\n",
    "    # turn off the bounding box\n",
    "    ax.axis(\"off\")\n",
    "    # plot the strings\n",
    "    for i in range(num):\n",
    "        ax.text(0, i, strs[i], fontsize = 12, ha = \"center\", va = \"center\")\n",
    "    plt.savefig(save)\n",
    "    plt.close()\n",
    "    \n",
    "def multi_reg_lineplot(file, #dataframe\n",
    "                       groups = [\"t1\", \"t2\", \"t3\"], #substring of header, group indicator\n",
    "                       labels = [\"0 min\", \"2 min\", \"5 min\"], # Goes above the strings\n",
    "                       log2_trans = True,\n",
    "                       savefile = \"test.pdf\", # path/to/file.pdf\n",
    "                       ):\n",
    "    \"\"\"\n",
    "    Takes in a Pandas dataframe as well as some groups and labels, then plots the multiple regression\n",
    "    output with LaTeX formatting. \n",
    "    \"\"\"\n",
    "    g_num = len(groups)\n",
    "    split_f = [file[[c for c in list(file.columns) if groups[i] in c]] for i in range(g_num)]\n",
    "    if log2_trans:\n",
    "        split_f = [[list(row) for row in list(g.astype(float).transform(np.log2).to_numpy())] for g in split_f]\n",
    "    else:\n",
    "        split_f = [[list(row) for row in list(g.astype(float).to_numpy())] for g in split_f]\n",
    "    # LinearRegression can't take missing values\n",
    "    split_f = [[row for row in g if all([item == item for item in row])] for g in split_f]\n",
    "    xs = [[row[:-1] for row in g] for g in split_f]\n",
    "    ys = [gh.transpose(*[[row[-1]] for row in g])[0] for g in split_f]\n",
    "    # Set up the model\n",
    "    linmods = [LinearRegression() for _ in range(g_num)]\n",
    "    # and fit it, always assume y is the last replicate in a group\n",
    "    regs = [linmods[i].fit(xs[i], ys[i]) for i in range(g_num)]\n",
    "    scores = [sqrt(regs[i].score(xs[i],ys[i])) for i in range(g_num)]\n",
    "    # Now we make the strings\n",
    "    strs = [format_linreg_strs(regs[i].coef_, scores[i], regs[i].intercept_, labels[i]) for i in range(g_num)]\n",
    "    # And pass them to the plotter\n",
    "    plot_linreg_strs(strs, save=savefile)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for plotting volcano plots and individual peptide heatmaps, wraps functions from mpl_plotting_helpers and a class from proteomics_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pepdep_volcano_arr(file_dfs,\n",
    "                       q_cols,\n",
    "                       fc_cols,\n",
    "                       wide = False,\n",
    "                       **volcano_kwargs):\n",
    "    # q-value column headers are the same from the standard peptide depot dump\n",
    "    # so get a list of all the q-values\n",
    "    all_qs = [[list(item) for item in list(df[q_cols].astype(float).to_numpy())] for df in file_dfs]\n",
    "    all_fc = [[list(item) for item in list(df[fc_cols].astype(float).to_numpy())] for df in file_dfs]\n",
    "    if wide:\n",
    "        all_qs = gh.transpose(*[gh.transpose(*row) for row in all_qs])\n",
    "        all_fc = gh.transpose(*[gh.transpose(*row) for row in all_fc])\n",
    "    mph.volcano_array(all_qs, all_fc, **volcano_kwargs)\n",
    "    return None\n",
    "\n",
    "def make_all_pepplots(peptide_list, path = \"outputs/graphics\",\n",
    "                      subset = [\"all\"], exclude = [], comparisons = [],\n",
    "                      foldchange_group = None,\n",
    "                      fc_max= None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_val = ceil(max([max(p.vals) for p in peptide_list]))\n",
    "    min_val = floor(min([min(p.vals) for p in peptide_list]))\n",
    "    hm_max = ceil(max([max(p.means) for p in peptide_list]))\n",
    "    hm_min = floor(min([min(p.means) for p in peptide_list]))\n",
    "    hm = [-max([abs(hm_min), abs(hm_max)]), max([abs(hm_min), abs(hm_max)])]\n",
    "    if fc_max == None:\n",
    "        fc_max = ceil(max([max(p.fc) for p in peptide_list]))\n",
    "        fc_min = floor(min([min(p.fc) for p in peptide_list]))\n",
    "        fc = [-max([abs(fc_min), abs(fc_max)]), max([abs(fc_min), abs(fc_max)])]\n",
    "    else:\n",
    "        fc = [-fc_max, fc_max]\n",
    "    for p in peptide_list:\n",
    "        skip = False\n",
    "        if not os.path.exists(f\"{path}/{p.gene.lower()}/heatmaps\"):\n",
    "            os.makedirs(f\"{path}/{p.gene.lower()}/heatmaps\")\n",
    "            p.heatmap(d_type = \"foldchange\",\n",
    "                  path = f\"{path}/{p.gene.lower()}/heatmaps/\", \n",
    "                  maxs = fc,\n",
    "                  subset = subset, exclude = exclude)\n",
    "        else:\n",
    "            p.heatmap(d_type = \"foldchange\",\n",
    "                          path = f\"{path}/{p.gene.lower()}/heatmaps/\", \n",
    "                          maxs = fc,\n",
    "                          subset = subset, exclude = exclude)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for plotting enrichment bubbleplots, will be moved to mpl_plotting_helpers eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_decoding = {\n",
    "    \"PERT-PSP\" : \"PhosphoSitePlus Perturbation\\nSignatures\",\n",
    "    \"DISEASE-PSP\" : \"PhosphoSitePlus Disease\\nSignatures\",\n",
    "    \"KINASE-PSP\" : \"PhosphoSitePlus Kinase\\nSignatures\",\n",
    "    \"KINASE-iKiP\" : \"In Vitro Kinase\\nSubstrate Signatures\",\n",
    "    \"PATH-BI\" : \"BI Pathway Signatures\",\n",
    "    \"PATH-NP\" : \"NetPath Pathway\\nSignatures\",\n",
    "    \"PATH-WP\" : \"WikiPathway Signatures\",\n",
    "    \"PERT-P100-DIA2\" : \"P100-DIA2\\nPertubation Signatures\",\n",
    "}\n",
    "\n",
    "def find_ids(str_list, delim = \"_\", position = 0, obj = []):\n",
    "    new_ids = {}\n",
    "    for s in str_list:\n",
    "        newid = s.split(delim)[position]\n",
    "        if newid not in list(new_ids.keys()):\n",
    "            new_ids[newid] = obj\n",
    "    return new_ids\n",
    "\n",
    "def split_by_dbtype(sea_hm_file, delim = \"_\", id_col = 0, position = 0):\n",
    "    # read the file, bin by id_col split on delim, return dict of sublists\n",
    "    parsed = find_ids(gh.transpose(*sea_hm_file)[0][1:], delim = delim, position = position)\n",
    "    parsed = {key : [sea_hm_file[0]] for key, value in parsed.items()}\n",
    "    for row in sea_hm_file[1:]:\n",
    "        newrow = [gh.list_to_str(row[0].split(delim)[1:], delimiter = \" \", newline = False)] + [float(item) for item in row[1:]]\n",
    "        parsed[row[id_col].split(delim)[position]].append(newrow)\n",
    "    return parsed\n",
    "\n",
    "def find_radius(sig, sig_mapper = {1 : 0.1,\n",
    "                                             0.1 : 0.2,\n",
    "                             0.05 : 0.3,\n",
    "                             0.005 : 0.49}):\n",
    "    if float(sig) != float(sig):\n",
    "        return 0\n",
    "    holder = 0.1\n",
    "    for thresh in list(sig_mapper.keys()):\n",
    "        if sig <= thresh:\n",
    "            holder = sig_mapper[thresh]\n",
    "    return holder\n",
    "\n",
    "def score_to_colour(score, low = 0, high = 0.99999999, max_score = 1, \n",
    "                    cmap = cm.get_cmap(\"cool\")):\n",
    "    if float(score) != float(score):\n",
    "        return \"grey\"\n",
    "    mid = (low+high)/2\n",
    "    moved_low = low - mid\n",
    "    moved_high = high - mid\n",
    "    scalar = moved_high/max_score\n",
    "    color = (score*scalar) + mid\n",
    "    return cmap(color)\n",
    "\n",
    "def all_scores_to_colours(score_matrix, **stc_kwargs):\n",
    "    # This should be a n (rows) x m (cols) matrix of scores (numbers)\n",
    "    return [[score_to_colour(score, **stc_kwargs) for score in row] for row in score_matrix]\n",
    "\n",
    "def find_all_radii(sig_matrix, sig_mapper = {1 : 0.1,\n",
    "                                             0.1 : 0.2,\n",
    "                             0.05 : 0.3,\n",
    "                             0.005 : 0.49}):\n",
    "    return [[find_radius(sig, sig_mapper = sig_mapper) for sig in row] for row in sig_matrix]\n",
    "\n",
    "def sea_arr_poss(w,h, colours = None, radii = None, labels = None):\n",
    "    if colours == None:\n",
    "        colours = [[\"white\" for j in range(w)] for i in range(h)]\n",
    "    if radii == None:\n",
    "        radii = [[0.1 for j in range(w)] for i in range(h)]\n",
    "    arr = [[Circle((i,j), radii[i][j],ec=\"black\", lw=0.5, color=colours[i][j]) \n",
    "            for j in range(w)] for i in range(h)]\n",
    "    return arr\n",
    "\n",
    "def legend_points(axis, sig_mapper, leg_scale, \n",
    "                  fontdict = dict(fontfamily=\"sans-serif\",\n",
    "                                      font = \"Arial\",\n",
    "                                      fontweight= \"bold\",\n",
    "                                      fontsize = 6)\n",
    "                 ):\n",
    "    # Make circles that are 0.5 apart with text next to them\n",
    "    circs = []\n",
    "    index = 0\n",
    "    for key, value in sig_mapper.items():\n",
    "        circs.append(Circle((1,index), radius = value, color = \"white\", ec = 'black' ) )\n",
    "        index += 1\n",
    "    keys = list(sig_mapper.keys())\n",
    "    for i in range(len(circs)):\n",
    "        axis.add_patch(circs[i])\n",
    "        axis.text(1.5, i, f\"$q < {keys[i]}$\",**fontdict)\n",
    "    return None\n",
    "\n",
    "def add_legend(ax1,ax2, cmap, vmin = -10, vmax = 10, \n",
    "               leg_scale = 69,\n",
    "               sig_mapper = {1 : 0.1,\n",
    "                                             0.1 : 0.2,\n",
    "                             0.05 : 0.3,\n",
    "                             0.005 : 0.49},\n",
    "               fontdict = dict(fontfamily=\"sans-serif\",\n",
    "                                      font = \"Arial\",\n",
    "                                      fontweight= \"bold\",\n",
    "                                      fontsize = 6),\n",
    "               **stc_kwargs):\n",
    "    # First, make fake circles that are white\n",
    "    sig_matr = [[1,0.05,0.005,0.0005,0.00005]]\n",
    "    rads= find_all_radii(sig_matr)\n",
    "    #points = sea_arr_poss(len(rads[0]), len(rads), labels = [[str(item) for item in row] for row in sig_matr])\n",
    "    img = plt.imshow([[-10,10]], cmap = cmap, vmin = vmin, vmax = vmax)\n",
    "    img.set_visible(False)\n",
    "    cb = plt.colorbar(ax = ax1, fraction = 0.046, pad = 0.04)\n",
    "    cb.set_ticks([item for item in cb.get_ticks() if item == int(item)])\n",
    "    cb.set_ticklabels([int(item) for item in cb.get_ticks()], **fontdict)\n",
    "    #cb.set_label(label, fontfamily = \"sans-serif\",\n",
    "    #                  font = \"Arial\", fontweight= \"bold\", loc = \"top\")\n",
    "    legend_points(ax2, sig_mapper, leg_scale, fontdict= fontdict)\n",
    "    return ax1,ax2\n",
    "\n",
    "def enrich_bubbleplot(enriched_dict,\n",
    "                      savefile, # Should equal len(keys), be a directory to put files\n",
    "                      filetype = \"pdf\",\n",
    "                      group_heads = [\"Large dong\" for _ in range(20)],\n",
    "                      bubblenum = 20, \n",
    "                      colourmap = mph.trans,\n",
    "                      max_score = 10,\n",
    "                      fontdict = dict(fontfamily=\"sans-serif\",\n",
    "                                      font = \"Arial\",\n",
    "                                      fontweight= \"bold\",\n",
    "                                      fontsize = 6),\n",
    "                      vmin = -10, vmax = 10,\n",
    "                      ):\n",
    "    # Make an enrichment bubbleplot for every grouping, containing only\n",
    "    # bubblenum groups.\n",
    "    # Any filtering should be done preemptively.\n",
    "    cmap = cm.get_cmap(colourmap)\n",
    "    rgba = cmap(0.4999999995)\n",
    "    # Loop over the keys and values of the dictionary\n",
    "    saver = 0\n",
    "    for key, value in enriched_dict.items():\n",
    "        # The 0th row is headers, so ignore them.We will use group_heads for this\n",
    "        # Grab the q-values and the enrichment scores. the number of\n",
    "        # q-cols and nes-cols = len(group_heads)\n",
    "        # Also, the 0th column is the labels\n",
    "        qs = [row[1:len(group_heads)+1] for row in value[1:]]\n",
    "        #qs = gh.transpose(*qs)\n",
    "        qs = [qs[bubblenum*i:bubblenum*(i+1)] for i in range(len(qs)//bubblenum + 1)]\n",
    "        qs = [q_list for q_list in qs if q_list != []]\n",
    "        # For some reason, the above listcomp can produce empty lists at the end,\n",
    "        # which breaks things downstream. So we'll just remove them\n",
    "        #qs = [gh.transpose(*c) for c in qs]\n",
    "        nes = [row[len(group_heads)+1:] for row in value[1:]]\n",
    "        #nes = gh.transpose(*nes)\n",
    "        nes = [nes[bubblenum*i:bubblenum*(i+1)] for i in range(len(nes)//bubblenum + 1)]\n",
    "        # For some reason, the above listcomp can produce empty lists at the end,\n",
    "        # which breaks things downstream. So we'll just remove them\n",
    "        nes = [nes_list for nes_list in nes if nes_list != []]\n",
    "        \n",
    "        #nes = [gh.transpose(*c) for c in nes]\n",
    "        labels = [row[0] for row in value[1:]]\n",
    "        labels = [labels[bubblenum*i:bubblenum*(i+1)] for i in range(len(labels)//bubblenum + 1)]\n",
    "        # The sea_arr_poss fails if there are no entries\n",
    "        points = []\n",
    "        #print(len(qs))\n",
    "        #for i in range(len(qs)):\n",
    "        #    print(qs[i])\n",
    "        #    points.append(sea_arr_poss(len(qs[i]),    # Rows\n",
    "        #                          len(qs[i][0]), # Cols\n",
    "        #                          colours = all_scores_to_colours(gh.transpose(*nes[i]),\n",
    "        #                                                          max_score=max_score,\n",
    "        #                                                          cmap=cmap),\n",
    "        #                          radii= find_all_radii(gh.transpose(*qs[i]))) )\n",
    "        try:\n",
    "            for i in range(len(qs)):\n",
    "                points.append(sea_arr_poss(len(qs[i]),    # Rows\n",
    "                                  len(qs[i][0]), # Cols\n",
    "                                  colours = all_scores_to_colours(gh.transpose(*nes[i]),\n",
    "                                                                  max_score=max_score,\n",
    "                                                                  cmap=cmap),\n",
    "                                  radii= find_all_radii(gh.transpose(*qs[i]))) )\n",
    "        except:\n",
    "            print(f\"Skipping category {database_decoding[key]}: No groups were enriched.\\n\")\n",
    "        if False:\n",
    "            continue\n",
    "        else:\n",
    "            index = 0\n",
    "            # Begin plotting each cluster\n",
    "            for cluster in points:\n",
    "                fig, ax = plt.subplots(1,2, figsize = (6,6))\n",
    "                for row in cluster:\n",
    "                    for p in row:\n",
    "                        ax[0].add_artist(p)\n",
    "                # Set some of the parameters\n",
    "                ax[0].set_xticks(list(range(len(group_heads))))\n",
    "                ax[0].set_xticklabels(group_heads, rotation = 90, **fontdict)\n",
    "                ax[0].set_yticks(list(range(len(labels[index]))))\n",
    "                ax[0].set_yticklabels(labels[index], **fontdict)\n",
    "                ax[0].set_xlim(-1, bubblenum+1)\n",
    "                ax[0].set_ylim(-1, bubblenum+1)\n",
    "                ax[1].set_xlim(-1, bubblenum+1)\n",
    "                ax[1].set_ylim(-1, bubblenum+1)\n",
    "                ax[0].set_aspect(\"equal\")\n",
    "                ax[1].set_aspect(\"equal\")\n",
    "                ax[1].axis('off')\n",
    "                ax[0].set_title(database_decoding[key],**fontdict)\n",
    "                add_legend(ax[0], ax[1], cmap, leg_scale = bubblenum,\n",
    "                          vmin = -max_score, vmax = max_score)\n",
    "                ax[0].spines[:].set_visible(False)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{savefile}/{key}_{index}.{filetype}\")\n",
    "                plt.close()\n",
    "                index +=1\n",
    "        saver += 1\n",
    "    return None\n",
    "\n",
    "def enrich_bubbleplot_list(ptmsea_outfiles, \n",
    "                           savefiles,\n",
    "                           sig_exception = [\"filenames\"],\n",
    "                           significance = 1, # 0 < significance < 1\n",
    "                           filetype = \"pdf\",\n",
    "                           group_heads = [\"Large dong\" for _ in range(20)],\n",
    "                           bubblenum = 15, \n",
    "                           colourmap = mph.trans,\n",
    "                           max_score = 10,\n",
    "                           fontdict = dict(fontfamily=\"sans-serif\",\n",
    "                                      font = \"Arial\",\n",
    "                                      fontweight= \"bold\",\n",
    "                                      fontsize = 6),\n",
    "                           vmin = -10, vmax = 10,):\n",
    "    # Wraps bubbleplot to do a list of them and send them where they need to go\n",
    "    for i in range(len(ptmsea_outfiles)):\n",
    "        print(ptmsea_outfiles[i])\n",
    "        file = gh.read_file(ptmsea_outfiles[i])\n",
    "        file = split_by_dbtype(file)\n",
    "        if ptmsea_outfiles[i] in sig_exception:\n",
    "            print(\"exception\")\n",
    "            file = {key : [value[0]] + [row for row in value[1:] if any([item < 1 for item in row[1:len(group_heads)+1]])] \n",
    "               for key, value in file.items()}\n",
    "        else:\n",
    "            file = {key : [value[0]] + [row for row in value[1:] if any([item < significance for item in row[1:len(group_heads)+1]])]\n",
    "               for key, value in file.items()}\n",
    "            #print(file[\"PATH-NP\"])\n",
    "        # Remove any empties\n",
    "        file = {key : value for key, value in file.items() if len(value) >1}\n",
    "        enrich_bubbleplot(file, savefiles[i],\n",
    "                         filetype = filetype,\n",
    "                         group_heads = group_heads,\n",
    "                         bubblenum = bubblenum,\n",
    "                         colourmap = colourmap,\n",
    "                          max_score = max_score,\n",
    "                          fontdict = fontdict,\n",
    "                          vmin = vmin, vmax = vmax)\n",
    "        #break\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells perform these actions:\n",
    "\n",
    "<ul>\n",
    "    <li>Reading the files in and making dataframes using the read_and_filter() function </li>\n",
    "    <li>Turning the files into lists and formatting for PTM-SEA</li>\n",
    "    <li>Writing the GCT files</li>\n",
    "    <li>Running multiple linear regression</li>\n",
    "    <li>Running PCA/NMF</li>\n",
    "    <li>Making volcano plots</li>\n",
    "    <li>Making individual plots for each phosphorylation site</li>\n",
    "    <li>Running PTM-SEA</li>\n",
    "    <li>Making PTM-SEA dot plots</li>\n",
    "</ul>\n",
    "\n",
    "For specific steps, more description will be added later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab the excel files and manage the data, including flanking sequences\n",
    "file_dfs = [read_and_filter(f, columns) for f in files]\n",
    "file_lists = [[list(row) for row in list(df.to_numpy())] for df in file_dfs]\n",
    "file_lists = [ [fcond_cols] + dup_flanks_matrix(f[1:], [20,21,22]) for f in file_lists]\n",
    "file_lists = [[row for row in f if row[20] == row[20]] for f in file_lists]\n",
    "file_lists = [[row + [ptmsea_transform(row[16], row[14], \"2m_vs_0m\"),\n",
    "                      ptmsea_transform(row[17], row[15], \"5m_vs_0m\")] for row in f]\n",
    "             for f in file_lists]\n",
    "file_lists = [[f[0]] + sorted(f[1:], key = lambda x: (x[20], x[23])) for f in file_lists]\n",
    "gct_inds = [[f\"{row[20]}-p\" for row in f[1:]] for f in file_lists]\n",
    "gct_vals = [[row[-2:] for row in f[1:]] for f in file_lists]\n",
    "gct_heads = [f[0][-2:] for f in file_lists]\n",
    "# Finally, write the GCT files for PTM-SEA later\n",
    "for i in range(len(file_lists)):\n",
    "    write_gct_file(gct_inds[i], gct_vals[i], gct_heads[i], gct_outnames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple regression and dump the equations/metrics\n",
    "narnar = [multi_reg_lineplot(file_dfs[i], \n",
    "                             groups = reg_gs, \n",
    "                             savefile = reg_outnames[i]) for i in range(len(file_dfs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA clustering using the file dataframes.\n",
    "pca_ax = cluster_plotting(file_dfs, # list with minimum 1 df\n",
    "                 [\"thresholded timepoint1\", \"thresholded timepoint2\", \"thresholded timepoint3\"],     \n",
    "                 exp_names,\n",
    "                 pca_outnames,\n",
    "                 [slice(0,4), slice(4,8), slice(8,12)],\n",
    "                 [\"0 min\", \"2 min\", \"5 min\"],\n",
    "                 colours,\n",
    "                 markers = [\"o\",\"^\", \"s\"],\n",
    "                 cluster = 'PCA',\n",
    "                 markersize=100,\n",
    "                 textdict = dict(fontfamily = \"sans-serif\",\n",
    "                 font = \"Arial\",\n",
    "                 fontweight = \"bold\",\n",
    "                 fontsize = 10),\n",
    "                 pca_kwargs = dict(n_components = 2,\n",
    "                                   whiten = False,\n",
    "                                   svd_solver = \"full\",\n",
    "                                   tol = 0),\n",
    "                 nmf_kwargs = dict(n_components = 2, \n",
    "                                   init = \"nndsvd\", # preserves sparseness\n",
    "                                   solver = \"mu\", # multiplicative update\n",
    "                                   beta_loss = \"frobenius\", # stable, but slow\n",
    "                                   alpha_W = 0,  # default\n",
    "                                   alpha_H = 0,  # default\n",
    "                                   l1_ratio = 0,  # default\n",
    "                                   max_iter = 100000\n",
    "                                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the dataframes and use the q-values and ratios\n",
    "# to create volcano plots\n",
    "for i in range(len(file_dfs)):\n",
    "    pepdep_volcano_arr([file_dfs[i]],\n",
    "                   [\"qvalues for SILAC timepoint1\",\n",
    "                    \"qvalues for SILAC timepoint2\"],\n",
    "                   [\"SILAC ratio 23 for user selected SILAC timepoint1\",\n",
    "                    \"SILAC ratio 23 for user selected SILAC timepoint2\"],\n",
    "                    wide = True,\n",
    "                   left_labels = leftlabs,\n",
    "                   right_labels = rightlabs,\n",
    "                   bottom_labels = bottomlabs,\n",
    "                   colours = [[colours[i]],[colours[i]]],\n",
    "                   top_labels = toplabs[i],\n",
    "                   xlim = 10, ylim = 4,\n",
    "                   fc_transform = \"log2\",\n",
    "                   sig_transform = \"log10\",\n",
    "                   fc_label_transform = \"none\",\n",
    "                   sig_cutoffs = sigs,\n",
    "                   fc_cutoff = 1,\n",
    "                   save = volc_outnames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all of the individual heatmaps for pY sites. This\n",
    "# goes pretty quick\n",
    "all_peps = [[Peptide(list(file_dfs[j].iloc[i][[col for col in list(file_dfs[j].columns) if \"rep\" in col]].astype(float)), \n",
    "                   [\"0m R1\", \"0m R2\", \"0m R3\", \"0m R4\",\n",
    "                    \"2m R1\", \"2m R2\", \"2m R3\", \"2m R4\",\n",
    "                    \"5m R1\", \"5m R2\", \"5m R3\", \"5m R4\"],\n",
    "                    [\"0m\", \"2m\", \"5m\"],\n",
    "                    file_dfs[j].iloc[i][\"assigned sequence\"],\n",
    "                    statistics = list(file_dfs[j].iloc[i][[col for col in list(file_dfs[j].columns) if \"qvalue\" in col]].astype(float)),\n",
    "                    statistics_headers = [\"2m vs 0m qvalue\", \"5m vs 0m qvalue\"],\n",
    "                     foldchange = list(np.log2(file_dfs[j].iloc[i][[col for col in list(file_dfs[j].columns) if \"ratio\" in col]].astype(float))),\n",
    "                    foldchange_headers = [\"2m vs 0m\", \"5m vs 0m\"],\n",
    "                     sites = fr'$^{{{file_dfs[j].iloc[i][\"phosphosite annotated\"]}}}$',\n",
    "                     gene = str(file_dfs[j].iloc[i][\"UNIPROT Gene Name\"]),\n",
    "                     unique_id = str(file_dfs[j].iloc[i][\"U_ID\"]),\n",
    "                     colours = colours[j],\n",
    "                     markers = [\"s\", \"o\", \"D\"]) \n",
    "            for i in range(len(file_dfs[j])) if str(file_dfs[j].iloc[i][\"UNIPROT Gene Name\"]) != str(\"nan\")]\n",
    "            for j in range(len(file_dfs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually create all of the heatmaps using the\n",
    "# Peptide objects. This takes forever, so many\n",
    "# files are created.\n",
    "\n",
    "for path in site_outnames:\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "i=0\n",
    "for peps in all_peps:\n",
    "    print(site_outnames[i])\n",
    "    make_all_pepplots(peps, path = site_outnames[i],\n",
    "                  subset = [\"m\"], exclude = [\"30m\"],\n",
    "                  comparisons = [\"2m vs 0m qvalue\",\n",
    "                                 \"5m vs 0m qvalue\"],\n",
    "                  foldchange_group = \" 0m\",\n",
    "                      fc_max = 6)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PTM-SEA. Also takes a while because subprocess is slow\n",
    "\n",
    "gsf.imp_main(\"/mnt/c/Users/redas/Desktop/jupyter_directory/salomon_lab_folder/car_target_project\",\n",
    "             \"qvalue\",\n",
    "             \"/mnt/c/Users/redas/Desktop/jupyter_directory/salomon_lab_folder/car_target_project/r_scripts/ssGSEA2.0.R\",\n",
    "             \"/mnt/c/Users/redas/Desktop/jupyter_directory/salomon_lab_folder/car_target_project/databases/ptm.sig.db.all.flanking.human.v2.0.0.gmt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all the nonsense from the PTM-SEA output, keep\n",
    "# only the enrichment scores, q-values, and groups\n",
    "mso.imp_main(\"/mnt/c/Users/redas/Desktop/jupyter_directory/salomon_lab_folder/car_target_project\",\n",
    "             \"output_combined.gct\",\n",
    "             \"\\t\",\n",
    "             \"output_combined_heatmap.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the bubbleplots :)\n",
    "enrich_bubbleplot_list(ptm_files, outdirs,\n",
    "                       sig_exception = [\"ween\"],\n",
    "                                       #[\"./skbr3_28bbz/pY_data/output_combined_heatmap.txt\",\n",
    "                                       # \"./skbr3_28bbz/pY_data/output_combined_heatmap.txt\",],\n",
    "                       significance = 0.2,\n",
    "                       group_heads = [\"2 min\", \"5 min\"],\n",
    "                       max_score = 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
